{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from scipy.linalg import cholesky, cho_solve, solve_triangular\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.utils.deprecation import deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Gaussian process regression (GPR).\n",
    "    The implementation is based on Algorithm 2.1 of Gaussian Processes\n",
    "    for Machine Learning (GPML) by Rasmussen and Williams.\n",
    "    In addition to standard scikit-learn estimator API,\n",
    "    GaussianProcessRegressor:\n",
    "       * allows prediction without prior fitting (based on the GP prior)\n",
    "       * provides an additional method sample_y(X), which evaluates samples\n",
    "         drawn from the GPR (prior or posterior) at given inputs\n",
    "       * exposes a method log_marginal_likelihood(theta), which can be used\n",
    "         externally for other ways of selecting hyperparameters, e.g., via\n",
    "         Markov chain Monte Carlo.\n",
    "    Read more in the :ref:`User Guide <gaussian_process>`.\n",
    "    .. versionadded:: 0.18\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : kernel object\n",
    "        The kernel specifying the covariance function of the GP. If None is\n",
    "        passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n",
    "        the kernel's hyperparameters are optimized during fitting.\n",
    "    alpha : float or array-like, optional (default: 1e-10)\n",
    "        Value added to the diagonal of the kernel matrix during fitting.\n",
    "        Larger values correspond to increased noise level in the observations.\n",
    "        This can also prevent a potential numerical issue during fitting, by\n",
    "        ensuring that the calculated values form a positive definite matrix.\n",
    "        If an array is passed, it must have the same number of entries as the\n",
    "        data used for fitting and is used as datapoint-dependent noise level.\n",
    "        Note that this is equivalent to adding a WhiteKernel with c=alpha.\n",
    "        Allowing to specify the noise level directly as a parameter is mainly\n",
    "        for convenience and for consistency with Ridge.\n",
    "    optimizer : string or callable, optional (default: \"fmin_l_bfgs_b\")\n",
    "        Can either be one of the internally supported optimizers for optimizing\n",
    "        the kernel's parameters, specified by a string, or an externally\n",
    "        defined optimizer passed as a callable. If a callable is passed, it\n",
    "        must have the signature::\n",
    "            def optimizer(obj_func, initial_theta, bounds):\n",
    "                # * 'obj_func' is the objective function to be maximized, which\n",
    "                #   takes the hyperparameters theta as parameter and an\n",
    "                #   optional flag eval_gradient, which determines if the\n",
    "                #   gradient is returned additionally to the function value\n",
    "                # * 'initial_theta': the initial value for theta, which can be\n",
    "                #   used by local optimizers\n",
    "                # * 'bounds': the bounds on the values of theta\n",
    "                ....\n",
    "                # Returned are the best found hyperparameters theta and\n",
    "                # the corresponding value of the target function.\n",
    "                return theta_opt, func_min\n",
    "        Per default, the 'fmin_l_bfgs_b' algorithm from scipy.optimize\n",
    "        is used. If None is passed, the kernel's parameters are kept fixed.\n",
    "        Available internal optimizers are::\n",
    "            'fmin_l_bfgs_b'\n",
    "    n_restarts_optimizer : int, optional (default: 0)\n",
    "        The number of restarts of the optimizer for finding the kernel's\n",
    "        parameters which maximize the log-marginal likelihood. The first run\n",
    "        of the optimizer is performed from the kernel's initial parameters,\n",
    "        the remaining ones (if any) from thetas sampled log-uniform randomly\n",
    "        from the space of allowed theta-values. If greater than 0, all bounds\n",
    "        must be finite. Note that n_restarts_optimizer == 0 implies that one\n",
    "        run is performed.\n",
    "    normalize_y : boolean, optional (default: False)\n",
    "        Whether the target values y are normalized, i.e., the mean of the\n",
    "        observed target values become zero. This parameter should be set to\n",
    "        True if the target values' mean is expected to differ considerable from\n",
    "        zero. When enabled, the normalization effectively modifies the GP's\n",
    "        prior based on the data, which contradicts the likelihood principle;\n",
    "        normalization is thus disabled per default.\n",
    "    copy_X_train : bool, optional (default: True)\n",
    "        If True, a persistent copy of the training data is stored in the\n",
    "        object. Otherwise, just a reference to the training data is stored,\n",
    "        which might cause predictions to change if the data is modified\n",
    "        externally.\n",
    "    random_state : int, RandomState instance or None, optional (default: None)\n",
    "        The generator used to initialize the centers. If int, random_state is\n",
    "        the seed used by the random number generator; If RandomState instance,\n",
    "        random_state is the random number generator; If None, the random number\n",
    "        generator is the RandomState instance used by `np.random`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_train_ : array-like, shape = (n_samples, n_features)\n",
    "        Feature values in training data (also required for prediction)\n",
    "    y_train_ : array-like, shape = (n_samples, [n_output_dims])\n",
    "        Target values in training data (also required for prediction)\n",
    "    kernel_ : kernel object\n",
    "        The kernel used for prediction. The structure of the kernel is the\n",
    "        same as the one passed as parameter but with optimized hyperparameters\n",
    "    L_ : array-like, shape = (n_samples, n_samples)\n",
    "        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``\n",
    "    alpha_ : array-like, shape = (n_samples,)\n",
    "        Dual coefficients of training data points in kernel space\n",
    "    log_marginal_likelihood_value_ : float\n",
    "        The log-marginal-likelihood of ``self.kernel_.theta``\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel=None, alpha=1e-10,\n",
    "                 optimizer=\"fmin_l_bfgs_b\", n_restarts_optimizer=0,\n",
    "                 normalize_y=False, copy_X_train=True, random_state=None):\n",
    "        self.kernel = kernel\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = optimizer\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.normalize_y = normalize_y\n",
    "        self.copy_X_train = copy_X_train\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def y_train_mean(self):\n",
    "        return self._y_train_mean\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Gaussian process regression model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape = (n_samples, [n_output_dims])\n",
    "            Target values\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        if self.kernel is None:  # Use an RBF kernel as default\n",
    "            self.kernel_ = C(1.0, constant_value_bounds=\"fixed\") \\\n",
    "                * RBF(1.0, length_scale_bounds=\"fixed\")\n",
    "        else:\n",
    "            self.kernel_ = clone(self.kernel)\n",
    "\n",
    "        self._rng = check_random_state(self.random_state)\n",
    "\n",
    "        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)\n",
    "\n",
    "        # Normalize target value\n",
    "        if self.normalize_y:\n",
    "            self._y_train_mean = np.mean(y, axis=0)\n",
    "            # demean y\n",
    "            y = y - self._y_train_mean\n",
    "        else:\n",
    "            self._y_train_mean = np.zeros(1)\n",
    "\n",
    "        if np.iterable(self.alpha) \\\n",
    "           and self.alpha.shape[0] != y.shape[0]:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha[0]\n",
    "            else:\n",
    "                raise ValueError(\"alpha must be a scalar or an array\"\n",
    "                                 \" with same number of entries as y.(%d != %d)\"\n",
    "                                 % (self.alpha.shape[0], y.shape[0]))\n",
    "\n",
    "        self.X_train_ = np.copy(X) if self.copy_X_train else X\n",
    "        self.y_train_ = np.copy(y) if self.copy_X_train else y\n",
    "\n",
    "        if self.optimizer is not None and self.kernel_.n_dims > 0:\n",
    "            # Choose hyperparameters based on maximizing the log-marginal\n",
    "            # likelihood (potentially starting from several initial values)\n",
    "            def obj_func(theta, eval_gradient=True):\n",
    "                if eval_gradient:\n",
    "                    lml, grad = self.log_marginal_likelihood(\n",
    "                        theta, eval_gradient=True)\n",
    "                    return -lml, -grad\n",
    "                else:\n",
    "                    return -self.log_marginal_likelihood(theta)\n",
    "\n",
    "            # First optimize starting from theta specified in kernel\n",
    "            optima = [(self._constrained_optimization(obj_func,\n",
    "                                                      self.kernel_.theta,\n",
    "                                                      self.kernel_.bounds))]\n",
    "\n",
    "            # Additional runs are performed from log-uniform chosen initial\n",
    "            # theta\n",
    "            if self.n_restarts_optimizer > 0:\n",
    "                if not np.isfinite(self.kernel_.bounds).all():\n",
    "                    raise ValueError(\n",
    "                        \"Multiple optimizer restarts (n_restarts_optimizer>0) \"\n",
    "                        \"requires that all bounds are finite.\")\n",
    "                bounds = self.kernel_.bounds\n",
    "                for iteration in range(self.n_restarts_optimizer):\n",
    "                    theta_initial = \\\n",
    "                        self._rng.uniform(bounds[:, 0], bounds[:, 1])\n",
    "                    optima.append(\n",
    "                        self._constrained_optimization(obj_func, theta_initial,\n",
    "                                                       bounds))\n",
    "            # Select result from run with minimal (negative) log-marginal\n",
    "            # likelihood\n",
    "            lml_values = list(map(itemgetter(1), optima))\n",
    "            self.kernel_.theta = optima[np.argmin(lml_values)][0]\n",
    "            self.log_marginal_likelihood_value_ = -np.min(lml_values)\n",
    "        else:\n",
    "            self.log_marginal_likelihood_value_ = \\\n",
    "                self.log_marginal_likelihood(self.kernel_.theta)\n",
    "\n",
    "        # Precompute quantities required for predictions which are independent\n",
    "        # of actual query points\n",
    "        K = self.kernel_(self.X_train_)\n",
    "        K[np.diag_indices_from(K)] += self.alpha\n",
    "        try:\n",
    "            self.L_ = cholesky(K, lower=True)  # Line 2\n",
    "        except np.linalg.LinAlgError as exc:\n",
    "            exc.args = (\"The kernel, %s, is not returning a \"\n",
    "                        \"positive definite matrix. Try gradually \"\n",
    "                        \"increasing the 'alpha' parameter of your \"\n",
    "                        \"GaussianProcessRegressor estimator.\"\n",
    "                        % self.kernel_,) + exc.args\n",
    "            raise\n",
    "        self.alpha_ = cho_solve((self.L_, True), self.y_train_)  # Line 3\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, return_std=False, return_cov=False):\n",
    "        \"\"\"Predict using the Gaussian process regression model\n",
    "        We can also predict based on an unfitted model by using the GP prior.\n",
    "        In addition to the mean of the predictive distribution, also its\n",
    "        standard deviation (return_std=True) or covariance (return_cov=True).\n",
    "        Note that at most one of the two can be requested.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Query points where the GP is evaluated\n",
    "        return_std : bool, default: False\n",
    "            If True, the standard-deviation of the predictive distribution at\n",
    "            the query points is returned along with the mean.\n",
    "        return_cov : bool, default: False\n",
    "            If True, the covariance of the joint predictive distribution at\n",
    "            the query points is returned along with the mean\n",
    "        Returns\n",
    "        -------\n",
    "        y_mean : array, shape = (n_samples, [n_output_dims])\n",
    "            Mean of predictive distribution a query points\n",
    "        y_std : array, shape = (n_samples,), optional\n",
    "            Standard deviation of predictive distribution at query points.\n",
    "            Only returned when return_std is True.\n",
    "        y_cov : array, shape = (n_samples, n_samples), optional\n",
    "            Covariance of joint predictive distribution a query points.\n",
    "            Only returned when return_cov is True.\n",
    "        \"\"\"\n",
    "        if return_std and return_cov:\n",
    "            raise RuntimeError(\n",
    "                \"Not returning standard deviation of predictions when \"\n",
    "                \"returning full covariance.\")\n",
    "#         try:\n",
    "#             X = check_array(X)\n",
    "#         except:\n",
    "            \n",
    "\n",
    "        if not hasattr(self, \"X_train_\"):  # Unfitted;predict based on GP prior\n",
    "            if self.kernel is None:\n",
    "                kernel = (C(1.0, constant_value_bounds=\"fixed\") *\n",
    "                          RBF(1.0, length_scale_bounds=\"fixed\"))\n",
    "            else:\n",
    "                kernel = self.kernel\n",
    "            y_mean = np.zeros(X.shape[0])\n",
    "            if return_cov:\n",
    "                y_cov = kernel(X)\n",
    "                return y_mean, y_cov\n",
    "            elif return_std:\n",
    "                y_var = kernel.diag(X)\n",
    "                return y_mean, np.sqrt(y_var)\n",
    "            else:\n",
    "                return y_mean\n",
    "        else:  # Predict based on GP posterior\n",
    "            K_trans = self.kernel_(X, self.X_train_)\n",
    "            y_mean = K_trans.dot(self.alpha_)  # Line 4 (y_mean = f_star)\n",
    "            y_mean = self._y_train_mean + y_mean  # undo normal.\n",
    "            if return_cov:\n",
    "                v = cho_solve((self.L_, True), K_trans.T)  # Line 5\n",
    "                y_cov = self.kernel_(X) - K_trans.dot(v)  # Line 6\n",
    "                return y_mean, y_cov\n",
    "            elif return_std:\n",
    "                # compute inverse K_inv of K based on its Cholesky\n",
    "                # decomposition L and its inverse L_inv\n",
    "                L_inv = solve_triangular(self.L_.T, np.eye(self.L_.shape[0]))\n",
    "                K_inv = L_inv.dot(L_inv.T)\n",
    "                # Compute variance of predictive distribution\n",
    "                y_var = self.kernel_.diag(X)\n",
    "                y_var -= np.einsum(\"ij,ij->i\", np.dot(K_trans, K_inv), K_trans)\n",
    "\n",
    "                # Check if any of the variances is negative because of\n",
    "                # numerical issues. If yes: set the variance to 0.\n",
    "                y_var_negative = y_var < 0\n",
    "                if np.any(y_var_negative):\n",
    "                    warnings.warn(\"Predicted variances smaller than 0. \"\n",
    "                                  \"Setting those variances to 0.\")\n",
    "                    y_var[y_var_negative] = 0.0\n",
    "                return y_mean, np.sqrt(y_var)\n",
    "            else:\n",
    "                return y_mean\n",
    "\n",
    "    def sample_y(self, X, n_samples=1, random_state=0):\n",
    "        \"\"\"Draw samples from Gaussian process and evaluate at X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples_X, n_features)\n",
    "            Query points where the GP samples are evaluated\n",
    "        n_samples : int, default: 1\n",
    "            The number of samples drawn from the Gaussian process\n",
    "        random_state : int, RandomState instance or None, optional (default=0)\n",
    "            If int, random_state is the seed used by the random number\n",
    "            generator; If RandomState instance, random_state is the\n",
    "            random number generator; If None, the random number\n",
    "            generator is the RandomState instance used by `np.random`.\n",
    "        Returns\n",
    "        -------\n",
    "        y_samples : array, shape = (n_samples_X, [n_output_dims], n_samples)\n",
    "            Values of n_samples samples drawn from Gaussian process and\n",
    "            evaluated at query points.\n",
    "        \"\"\"\n",
    "        rng = check_random_state(random_state)\n",
    "\n",
    "        y_mean, y_cov = self.predict(X, return_cov=True)\n",
    "        if y_mean.ndim == 1:\n",
    "            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T\n",
    "        else:\n",
    "            y_samples = \\\n",
    "                [rng.multivariate_normal(y_mean[:, i], y_cov,\n",
    "                                         n_samples).T[:, np.newaxis]\n",
    "                 for i in range(y_mean.shape[1])]\n",
    "            y_samples = np.hstack(y_samples)\n",
    "        return y_samples\n",
    "\n",
    "    def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "        \"\"\"Returns log-marginal likelihood of theta for training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : array-like, shape = (n_kernel_params,) or None\n",
    "            Kernel hyperparameters for which the log-marginal likelihood is\n",
    "            evaluated. If None, the precomputed log_marginal_likelihood\n",
    "            of ``self.kernel_.theta`` is returned.\n",
    "        eval_gradient : bool, default: False\n",
    "            If True, the gradient of the log-marginal likelihood with respect\n",
    "            to the kernel hyperparameters at position theta is returned\n",
    "            additionally. If True, theta must not be None.\n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : float\n",
    "            Log-marginal likelihood of theta for training data.\n",
    "        log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n",
    "            Gradient of the log-marginal likelihood with respect to the kernel\n",
    "            hyperparameters at position theta.\n",
    "            Only returned when eval_gradient is True.\n",
    "        \"\"\"\n",
    "        if theta is None:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Gradient can only be evaluated for theta!=None\")\n",
    "            return self.log_marginal_likelihood_value_\n",
    "\n",
    "        kernel = self.kernel_.clone_with_theta(theta)\n",
    "\n",
    "        if eval_gradient:\n",
    "            K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
    "        else:\n",
    "            K = kernel(self.X_train_)\n",
    "\n",
    "        K[np.diag_indices_from(K)] += self.alpha\n",
    "        try:\n",
    "            L = cholesky(K, lower=True)  # Line 2\n",
    "        except np.linalg.LinAlgError:\n",
    "            return (-np.inf, np.zeros_like(theta)) \\\n",
    "                if eval_gradient else -np.inf\n",
    "\n",
    "        # Support multi-dimensional output of self.y_train_\n",
    "        y_train = self.y_train_\n",
    "        if y_train.ndim == 1:\n",
    "            y_train = y_train[:, np.newaxis]\n",
    "\n",
    "        alpha = cho_solve((L, True), y_train)  # Line 3\n",
    "\n",
    "        # Compute log-likelihood (compare line 7)\n",
    "        log_likelihood_dims = -0.5 * np.einsum(\"ik,ik->k\", y_train, alpha)\n",
    "        log_likelihood_dims -= np.log(np.diag(L)).sum()\n",
    "        log_likelihood_dims -= K.shape[0] / 2 * np.log(2 * np.pi)\n",
    "        log_likelihood = log_likelihood_dims.sum(-1)  # sum over dimensions\n",
    "\n",
    "        if eval_gradient:  # compare Equation 5.9 from GPML\n",
    "            tmp = np.einsum(\"ik,jk->ijk\", alpha, alpha)  # k: output-dimension\n",
    "            tmp -= cho_solve((L, True), np.eye(K.shape[0]))[:, :, np.newaxis]\n",
    "            # Compute \"0.5 * trace(tmp.dot(K_gradient))\" without\n",
    "            # constructing the full matrix tmp.dot(K_gradient) since only\n",
    "            # its diagonal is required\n",
    "            log_likelihood_gradient_dims = \\\n",
    "                0.5 * np.einsum(\"ijl,ijk->kl\", tmp, K_gradient)\n",
    "            log_likelihood_gradient = log_likelihood_gradient_dims.sum(-1)\n",
    "\n",
    "        if eval_gradient:\n",
    "            return log_likelihood, log_likelihood_gradient\n",
    "        else:\n",
    "            return log_likelihood\n",
    "\n",
    "    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n",
    "        if self.optimizer == \"fmin_l_bfgs_b\":\n",
    "            theta_opt, func_min, convergence_dict = \\\n",
    "                fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)\n",
    "            if convergence_dict[\"warnflag\"] != 0:\n",
    "                warnings.warn(\"fmin_l_bfgs_b terminated abnormally with the \"\n",
    "                              \" state: %s\" % convergence_dict)\n",
    "        elif callable(self.optimizer):\n",
    "            theta_opt, func_min = \\\n",
    "                self.optimizer(obj_func, initial_theta, bounds=bounds)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer %s.\" % self.optimizer)\n",
    "\n",
    "        return theta_opt, func_min\n",
    "    \n",
    "    def mu_grad(self, x):\n",
    "        \n",
    "        mu = lambda x: self.predict(x)\n",
    "        auto_grad = autograd.grad(mu)\n",
    "        return auto_grad(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training data: (30, 1)\n",
      "X testing data: (100, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some toy data\n",
    "d_dimensions = 1\n",
    "n_samples = 20\n",
    "noise_std = 0.1\n",
    "seed = 123\n",
    "rs = np.random.RandomState(seed)\n",
    "\n",
    "n_train = 30\n",
    "n_test = 100\n",
    "xtrain = np.linspace(-4, 5, n_train).reshape(n_train, 1)\n",
    "xtest = np.linspace(-4, 5, n_test).reshape(n_test, 1)\n",
    "print('X training data:', xtrain.shape)\n",
    "print('X testing data:', xtest.shape)\n",
    "\n",
    "\n",
    "# Labels\n",
    "f = lambda x: np.sin(x) * np.exp(0.2 * x)\n",
    "ytrain = f(xtrain) + noise_std * np.random.randn(n_train, 1)\n",
    "ytest = f(xtest)\n",
    "# Plot the function\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "ax.scatter(xtrain, ytrain)\n",
    "ax.plot(xtest, ytest)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF(length_scale=1.59) + WhiteKernel(noise_level=0.00678)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2db4660aebfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmu_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-e60d79381f4a>\u001b[0m in \u001b[0;36mmu_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mauto_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mauto_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     should be scalar-valued. The gradient has the same type as the argument.\"\"\"\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVJPNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mend_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstart_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mend_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_box\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36munary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0msubargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-e60d79381f4a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmu_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mauto_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauto_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-e60d79381f4a>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0my_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predict based on GP posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mK_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Line 4 (y_mean = f_star)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_mean\u001b[0m  \u001b[0;31m# undo normal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mK1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK1_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                     \"Gradient can only be evaluated when Y is None.\")\n\u001b[1;32m   1214\u001b[0m             dists = cdist(X / length_scale, Y / length_scale,\n\u001b[0;32m-> 1215\u001b[0;31m                           metric='sqeuclidean')\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             XA, XB, typ, kwargs = _validate_cdist_input(XA, XB, mA, mB, n,\n\u001b[0;32m-> 2656\u001b[0;31m                                                         metric_name, **kwargs)\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[0;31m# get cdist wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             cdist_fn = getattr(_distance_wrap,\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_cdist_input\u001b[0;34m(XA, XB, mA, mB, n, metric_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mXA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# validate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mXA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mXB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_convert_to_type\u001b[0;34m(X, out_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mascontiguousarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, random_state=seed)\n",
    "\n",
    "# fit the gp model to the inputs and targets\n",
    "gp_model.fit(xtrain, ytrain)\n",
    "\n",
    "print(gp_model.kernel_)\n",
    "y_pred, sigma = gp_model.predict(xtest, return_std=True)\n",
    "mu_grad = gp_model.mu_grad(xtest)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(xtrain, ytrain)\n",
    "ax.scatter(xtrain, ytrain)\n",
    "ax.plot(xtest, y_pred)\n",
    "ax.plot(xtest, ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
